{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdcea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage import color\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a68fc7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817c46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_convs(in_channels, out_channels):\n",
    "    conv_layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same', bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same', bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "    return conv_layers\n",
    "\n",
    "def expansion_block(upsample_layer, conv_layer, inp, concat_inp):\n",
    "    mask = upsample_layer(inp)\n",
    "    mask = torch.concat([concat_inp, mask], dim=1)\n",
    "    mask = conv_layer(mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #contraction path\n",
    "        self.contrac1 = double_convs(in_channels, 64)\n",
    "        self.contrac2 = double_convs(64, 128)\n",
    "        self.contrac3 = double_convs(128, 256)\n",
    "        self.contrac4 = double_convs(256, 512)\n",
    "        self.contrac5 = double_convs(512, 1024)\n",
    "\n",
    "        #expansion path\n",
    "        self.upsample1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.double_conv1 = double_convs(1024, 512)\n",
    "        self.upsample2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.double_conv2 = double_convs(512, 256)\n",
    "        self.upsample3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.double_conv3 = double_convs(256, 128)\n",
    "        self.upsample4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.double_conv4 = double_convs(128, 64)\n",
    "\n",
    "        #output layer\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding='same'),\n",
    "            nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        #contraction\n",
    "        cntrc_out1 = self.contrac1(image) #->\n",
    "        out1 = self.max_pool(cntrc_out1)\n",
    "        \n",
    "        cntrc_out2 = self.contrac2(out1) #->\n",
    "        out2 = self.max_pool(cntrc_out2)\n",
    "\n",
    "        cntrc_out3 = self.contrac3(out2) #->\n",
    "        out3 = self.max_pool(cntrc_out3)\n",
    "\n",
    "        cntrc_out4 = self.contrac4(out3) #->\n",
    "        out4 = self.max_pool(cntrc_out4)\n",
    "\n",
    "        cntrc_out5 = self.contrac5(out4)\n",
    "\n",
    "        #expansion\n",
    "        mask = expansion_block(self.upsample1, self.double_conv1, cntrc_out5, cntrc_out4)\n",
    "        mask = expansion_block(self.upsample2, self.double_conv2, mask, cntrc_out3)\n",
    "        mask = expansion_block(self.upsample3, self.double_conv3, mask, cntrc_out2)\n",
    "        mask = expansion_block(self.upsample4, self.double_conv4, mask, cntrc_out1)\n",
    "\n",
    "        #output\n",
    "        output = self.out(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016e50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e11da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../model/model.pt\", map_location=\"cpu\")\n",
    "model_state = checkpoint['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29380739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96099581",
   "metadata": {},
   "source": [
    "## Transforming Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e50c0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    N_COLORS = 22\n",
    "    COLORS = np.array([[0.658463, 0.178962, 0.372748, 1.      ],\n",
    "       [0.428768, 0.09479 , 0.432412, 1.      ],\n",
    "       [0.615513, 0.161817, 0.391219, 1.      ],\n",
    "       [0.846709, 0.297559, 0.244113, 1.      ],\n",
    "       [0.447428, 0.101597, 0.43108 , 1.      ],\n",
    "       [0.50973 , 0.123769, 0.422156, 1.      ],\n",
    "       [0.002267, 0.00127 , 0.01857 , 1.      ],\n",
    "       [0.980824, 0.572209, 0.028508, 1.      ],\n",
    "       [0.64626 , 0.173914, 0.378359, 1.      ],\n",
    "       [0.964394, 0.843848, 0.273391, 1.      ],\n",
    "       [0.190367, 0.039309, 0.361447, 1.      ],\n",
    "       [0.949545, 0.955063, 0.50786 , 1.      ],\n",
    "       [0.60933 , 0.159474, 0.393589, 1.      ],\n",
    "       [0.522206, 0.12815 , 0.419549, 1.      ],\n",
    "       [0.087411, 0.044556, 0.224813, 1.      ],\n",
    "       [0.013995, 0.011225, 0.071862, 1.      ],\n",
    "       [0.004547, 0.003392, 0.030909, 1.      ],\n",
    "       [0.453651, 0.103848, 0.430498, 1.      ],\n",
    "       [0.962517, 0.851476, 0.285546, 1.      ],\n",
    "       [0.379001, 0.076253, 0.432719, 1.      ],\n",
    "       [0.553392, 0.139134, 0.411829, 1.      ],\n",
    "       [0.851384, 0.30226 , 0.239636, 1.      ]])\n",
    "    MODEL = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1afec1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs(image_arr):\n",
    "    transforms = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    trnsfrmd = transforms(image=image_arr)\n",
    "\n",
    "    image = trnsfrmd['image']\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f73736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, image):\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    logits = model(image.float())\n",
    "    probs = logits.softmax(axis=1)\n",
    "    probs = probs[0]\n",
    "    pred = torch.argmax(probs, axis=0)\n",
    "    pred *= 10\n",
    "    \n",
    "    return pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2a2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(frame):\n",
    "    md_input = transform_inputs(frame)\n",
    "    \n",
    "    pred_mask = get_output(config.MODEL, md_input)\n",
    "    \n",
    "    md_input = md_input.permute(1,2,0).numpy()\n",
    "    \n",
    "    result_image = color.label2rgb(pred_mask, md_input, colors=config.COLORS, alpha=0.7)\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdfe91",
   "metadata": {},
   "source": [
    "# converting video to frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b103319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(path):\n",
    "  \n",
    "    video = cv2.VideoCapture(path)\n",
    "  \n",
    "    # Used as counter variable\n",
    "    count = 0\n",
    "  \n",
    "    # checks whether frames were extracted\n",
    "    success = 1\n",
    "  \n",
    "    while success:\n",
    "  \n",
    "        success, image = video.read()\n",
    "        \n",
    "        if not success:\n",
    "            break\n",
    "        # Saves the frames with frame-count\n",
    "        cv2.imwrite(f\"../archive/frames/sample_vid/frame{count}.jpg\", image)\n",
    "  \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d820e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_frames(\"../archive/sample_vid.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13208f2b",
   "metadata": {},
   "source": [
    "# Predicting on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a99c66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../archive/videos/2. sample_data/input_frames\"\n",
    "frame_names = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38152a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0621626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'002054_leftImg8bit.png'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d93e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frames(frame_names, root_path):\n",
    "    frame_root_path = root_path + \"/input_frames\"\n",
    "    for frame in tqdm(frame_names):\n",
    "        frame_path = os.path.join(frame_root_path, frame)\n",
    "\n",
    "        img = Image.open(frame_path)\n",
    "        arr = np.array(img)\n",
    "\n",
    "        result_arr = get_results(arr)\n",
    "\n",
    "        result_img = Image.fromarray((result_arr*255).astype(np.uint8), 'RGB')\n",
    "        result_img = result_img.resize(img.size)\n",
    "\n",
    "        result_img.save(f\"{root_path}/result_images/{frame}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4007b6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012749195098876953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 99,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605f72f8866446de8755104a1371ca86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_path = \"../archive/videos/2. sample_data\"\n",
    "predict_frames(frame_names, root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda0eba",
   "metadata": {},
   "source": [
    "# Converting Frame to Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d63609ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(image_folder, video_name):      \n",
    "    images = os.listdir(image_folder)\n",
    "  \n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "  \n",
    "    # setting the frame width, height width\n",
    "    # the width, height of first image\n",
    "    height, width, layers = frame.shape\n",
    "  \n",
    "    video = cv2.VideoWriter(video_name, 0, 1, (width, height)) \n",
    "  \n",
    "    # Appending the images to the video one by one\n",
    "    for image in tqdm(images):\n",
    "        video.write(cv2.imread(os.path.join(image_folder, image))) \n",
    "      \n",
    "    # Deallocating memories taken for window creation\n",
    "\n",
    "    cv2.destroyAllWindows() \n",
    "    video.release()  # releasing the video generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c822a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"../archive/videos/0. sample_data/result_images\"\n",
    "video_name = '../archive/videos/0. sample_data/result_video1.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de99543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007514238357543945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 63,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33862355bb1498b86be1c10783b001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_video(image_folder, video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ce707",
   "metadata": {},
   "source": [
    "## Converting AVI to MP4\n",
    "Because Streamlit can only play mp4 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8133e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aeed9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ../app/tmpaxwelplm.mp4.\n",
      "Moviepy - Writing video ../app/tmpaxwelplm.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ../app/tmpaxwelplm.mp4\n"
     ]
    }
   ],
   "source": [
    "avi_file = \"../app/tmpaxwelplm.avi\"\n",
    "clip = moviepy.VideoFileClip(avi_file)\n",
    "mp4_file = avi_file[:-3]+\"mp4\"\n",
    "\n",
    "clip.write_videofile(mp4_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ab296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef1ce46de5a342ae95938da0650fa1526dd14d5aad3619351a4aa090629cba0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
